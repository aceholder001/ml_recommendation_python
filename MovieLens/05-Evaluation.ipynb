{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Evaluating Metrics:\n",
    "\n",
    "- Prediction Metrics (Similiar to Regression Problem)\n",
    "    - RMSE\n",
    "    - R2\n",
    "    - MAE\n",
    "    - Explained Variance\n",
    "\n",
    "\n",
    "### Hit Metrics (Similiar to Classification Metrics)\n",
    "**Hit** - defined by relevancy, a hit usually means whether the recommended \"k\" items hit the \"relevant\" items by the user. For example, a user may have clicked, viewed, or purchased an item for many times, and a hit in the recommended items indicate that the recommender performs well. Metrics like \"precision\", \"recall\", etc. measure the performance of such hitting accuracy.\n",
    "\n",
    "    - Precision@k\n",
    "    - Recall@k\n",
    "  \n",
    "\n",
    "### Ranking Metrics\n",
    "\n",
    "**Ranking** - ranking metrics give more explanations about, for the hitted items, whether they are ranked in a way that is preferred by the users whom the items will be recommended to. Metrics like \"mean average precision\", \"ndcg\", etc., evaluate whether the relevant items are ranked higher than the less-relevant or irrelevant items. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the environment path to find reco\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true = pd.DataFrame(\n",
    "        {\n",
    "            \"USER\": [1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
    "            \"ITEM\": [1, 2, 3, 1, 4, 5, 6, 7, 2, 5, 6, 8, 9, 10, 11, 12, 13, 14],\n",
    "            \"RATING\": [5, 4, 3, 5, 5, 3, 3, 1, 5, 5, 5, 4, 4, 3, 3, 3, 2, 1],\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_pred = pd.DataFrame(\n",
    "    {\n",
    "        \"USER\": [1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
    "        \"ITEM\": [3, 10, 12, 10, 3, 5, 11, 13, 4, 10, 7, 13, 1, 3, 5, 2, 11, 14],\n",
    "        \"RATING_PRED\": [14, 13, 12, 14, 13, 12, 11, 10, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER</th>\n",
       "      <th>ITEM</th>\n",
       "      <th>RATING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   USER  ITEM  RATING\n",
       "0     1     1       5\n",
       "1     1     2       4\n",
       "2     1     3       3\n",
       "3     2     1       5\n",
       "4     2     4       5"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_true.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_users = set(df_true.USER).intersection(set(df_pred.USER))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reco.evaluate import get_top_k_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_df(rating_true, rating_pred, k):\n",
    "    \n",
    "    # Make sure the prediction and true data frames have the same set of users\n",
    "    common_users = set(rating_true[\"USER\"]).intersection(set(rating_pred[\"USER\"]))\n",
    "    rating_true_common = rating_true[rating_true[\"USER\"].isin(common_users)]\n",
    "    rating_pred_common = rating_pred[rating_pred[\"USER\"].isin(common_users)]\n",
    "    n_users = len(common_users)\n",
    "\n",
    "    df_hit = get_top_k_items(rating_pred_common, \"USER\", \"RATING_PRED\", k)\n",
    "    df_hit = pd.merge(df_hit, rating_true_common, on=[\"USER\", \"ITEM\"])[\n",
    "        [\"USER\", \"ITEM\", \"rank\"]\n",
    "    ]\n",
    "\n",
    "    # count the number of hits vs actual relevant items per user\n",
    "    df_hit_count = pd.merge(\n",
    "        df_hit.groupby(\"USER\", as_index=False)[\"USER\"].agg({\"hit\": \"count\"}),\n",
    "        rating_true_common.groupby(\"USER\", as_index=False)[\"USER\"].agg(\n",
    "            {\"actual\": \"count\"}\n",
    "        ),\n",
    "        on=\"USER\",\n",
    "    )\n",
    "    \n",
    "    return df_hit, df_hit_count, n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(rating_true, rating_pred, k):\n",
    "    \n",
    "    df_hit, df_hit_count, n_users = get_hit_df(rating_true, rating_pred, k)\n",
    "    \n",
    "    if df_hit.shape[0] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return (df_hit_count[\"hit\"] / k).sum() / n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(rating_true, rating_pred, k):\n",
    "\n",
    "    df_hit, df_hit_count, n_users = get_hit_df(rating_true, rating_pred, k)\n",
    "\n",
    "    if df_hit.shape[0] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return (df_hit_count[\"hit\"] / df_hit_count[\"actual\"]).sum() / n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_at_k(df_true, df_pred, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2111111111111111"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_at_k(df_true, df_pred, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at_k(rating_true, rating_pred, k):\n",
    "\n",
    "    df_hit, df_hit_count, n_users = get_hit_df(rating_true, rating_pred, k)\n",
    "    \n",
    "    if df_hit.shape[0] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # calculate discounted gain for hit items\n",
    "    df_dcg = df_hit.copy()\n",
    "    # relevance in this case is always 1\n",
    "    df_dcg[\"dcg\"] = 1 / np.log1p(df_dcg[\"rank\"])\n",
    "    # sum up discount gained to get discount cumulative gain\n",
    "    df_dcg = df_dcg.groupby(\"USER\", as_index=False, sort=False).agg({\"dcg\": \"sum\"})\n",
    "    # calculate ideal discounted cumulative gain\n",
    "    df_ndcg = pd.merge(df_dcg, df_hit_count, on=[\"USER\"])\n",
    "    df_ndcg[\"idcg\"] = df_ndcg[\"actual\"].apply(\n",
    "        lambda x: sum(1 / np.log1p(range(1, min(x, k) + 1)))\n",
    "    )\n",
    "\n",
    "    # DCG over IDCG is the normalized DCG\n",
    "    return (df_ndcg[\"dcg\"] / df_ndcg[\"idcg\"]).sum() / n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33333333333333326"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_at_k(df_true, df_pred, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
